{"0": {
    "doc": "ğŸ“† Calendar",
    "title": "ğŸ“† Calendar",
    "content": "Lectures are held in SOLIS 104. All office hours are held in person. Office hours held from Monday through Friday are held in HalÄ±cÄ±oÄŸlu Data Science Institute 155. Saturday office hours, if any, will be held in a different location; see the corresponding calendar events for details. ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"1": {
    "doc": "ğŸ“Š Final Project",
    "title": "ğŸ“Š Final Project: The Data Science Lifecycle",
    "content": "Checkpoint 1 (1%) Due: Friday, February 28th at 11:59PM . Checkpoint 2 (1%) Due: Friday, March 7th at 11:59PM . Final Project (10%) Due: Friday, March 14th at 11:59PM . No extensions allowed on the final deadline . Welcome to Final Project, the final assignment of the quarter! ğŸ‘‹ . This project aims to be a culmination of everything youâ€™ve learned this quarter. In the project, you will conduct an open-ended investigation into one of the three datasets (Recipes and Ratings ğŸ½, League of Legends âŒ¨ï¸, or Power Outages ğŸ”‹). Specifically, youâ€™ll draw several visualizations to help understand the distributions of key variables, assess the missingness mechanisms of columns with missing values, test a hypotheses about the data, and finally, build and improve a predictive model. This project will be entirely manually graded by us â€“ thatâ€™s right, no autograders! . Final Project worth 10% in your overall grade, which means itâ€™s worth double what previous projects were worth. There are also two checkpoints, each worth 1%. As your final deliverables, youâ€™ll submit two things to us: a public-facing website as well as a PDF of your Jupyter Notebook. We encourage you to build something you are proud of as this will give you something concrete to put on your resume and show to potential employers! . Final Project is due on Friday, March 14th at 11:59PM. This is a hard deadline; you may NOT use the extension on this project. This is because we need to start grading projects right when you turn them in, so that there is enough time for you to make regrade requests before we submit grades to campus. The Final Project does also have two checkpoints, which is due on Friday, February 28th and Friday, March 7th. The Final Project Checkpoints are structured differently than other project checkpoints; rather than having you submit any code, youâ€™ll answer a few questions about your progress on the project. More details are in the Checkpoint Submission towards the bottom of this page. You can submit the checkpoint 1 and checkpoint 2 on Gradescope; make sure to tag your partner if you have one. The project is broken into two parts: . | Part 1: An analysis, submitted as a Jupyter Notebook. This will contain the details of your work. Focus on completing your analysis before moving to Part 2, as the analysis is the bulk of the project. | Part 2: A report, submitted as a website. This will contain a narrative â€œstoryâ€ with visuals. Focus on this after finishing most of your analysis. | . ",
    "url": "/proj04/#-final-project-the-data-science-lifecycle",
    "relUrl": "/proj04/#-final-project-the-data-science-lifecycle"
  },"2": {
    "doc": "ğŸ“Š Final Project",
    "title": "Table of Contents",
    "content": ". | Choosing a Dataset | Part 1: Analysis . | Step 1: Introduction | Step 2: Data Cleaning and Exploratory Data Analysis | Step 3: Assessment of Missingness | Step 4: Hypothesis Testing | Step 5: Framing a Prediction Problem | Step 6: Baseline Model | Step 7: Final Model | Step 8: Fairness Analysis | Style | . | Part 2: Report . | Step 1: Initializing a Jekyll GitHub Pages Site | Step 2: Choosing a Theme | Step 3: Embedding Content | Local Setup | . | Submission and Rubric . | Checkpoint 1 Submission | Checkpoint 2 Submission | Final Submission | Rubric | . | . ",
    "url": "/proj04/#table-of-contents",
    "relUrl": "/proj04/#table-of-contents"
  },"3": {
    "doc": "ğŸ“Š Final Project",
    "title": "Choosing a Dataset",
    "content": "In this project, you will perform an open-ended investigation into a single dataset. You must choose one of the following three datasets. Recipes and Ratings ğŸ½ Â Â  League of Legends âŒ¨ï¸ Â Â  Power Outages ğŸ”‹ . The dataset description pages linked above each have three sections: . | Getting the Data: Describes how to access the data and, in some cases, what various features mean. (In general, youâ€™re going to have to understand what your data means on your own!) | Example Questions and Prediction Problems: Example questions to explore in Part 1: Steps 1-4, and example prediction problems to build models for in Part 1: Steps 5-8. Use these as inspiration, but feel free to come up with your own questions and prediction problems! | Special Considerations: Things to be aware of when working with the given dataset, e.g. some additional requirements. | . When selecting which dataset you are going to use for your project, try choosing the one whose topic appeals to you the most as that will make finishing the project a lot more enjoyable. To help contextualize the kinds of analysis you can do in this project, it might help to look at these examples from Spring 2023. These examples offer insights into crafting effective research questions, but bear in mind that they have their unique strengths and weaknesses. Treat them as a foundation for inspiration, but donâ€™t just repeat or copy their work â€“ be original! . | League of Legends First Blood Statistical Analysis: This project excelled in clarifying their research aims, making the study understandable to a broader audience. In your own project, ensure that you provide a lucid and detailed explanation of your research focus. | Analyzing Power Outages: This project presents a noval way to do the data visualization. In your project, please think about what is the best way to present your data. | Investigation on the Relationship Between Amount of Sugar and Rating of Recipes: In this project, the students have effectively highlighted the improvement they made based on the baseline model. In your own project, also ensure that you explain what you have done to improve your baseline model. | . Before choosing a dataset, read the rest of this page to see whatâ€™s required of you! . ",
    "url": "/proj04/#choosing-a-dataset",
    "relUrl": "/proj04/#choosing-a-dataset"
  },"4": {
    "doc": "ğŸ“Š Final Project",
    "title": "Part 1: Analysis",
    "content": "Before beginning your analysis, youâ€™ll need to set up a few things. | Pull the latest version of the dsc80-2025-wi repo. Within the projects/project04 folder, there is a template.ipynb notebook that you will use as a template for the project. If you delete the file or want another copy of the template, you can re-download it from here. This is where your analysis will live; you will submit this entire notebook to us. | Select one of the three datasets mentioned above, download it, and load it into your template notebook. | . Once you have your dataset loaded in your notebook, itâ€™s time for you to find meaning in the real-world data youâ€™ve collected! Follow the steps below. For each step, we specify what must be done in your notebook and what must go on your website, which we expand on in Part 2. We recommend you write everything in your notebook first, and then move things over to your website once youâ€™ve completed your analysis. In Steps 1-4, youâ€™ll develop a deeper understanding of your dataset while trying to answer a single question. Step 1: Introduction . | Step | Analysis in Notebook | Report on Website | . | Introduction and Question Identification | Understand the data you have access to. Brainstorm a few questions that interest you about the dataset. Pick one question you plan to investigate further. (As the data science lifecycle tells us, this question may change as you work on your project.) | Provide an introduction to your dataset, and clearly state the one question your project is centered around. Why should readers of your website care about the dataset and your question specifically? Report the number of rows in the dataset, the names of the columns that are relevant to your question, and descriptions of those relevant columns. | . Step 2: Data Cleaning and Exploratory Data Analysis . | Step | Analysis in Notebook | Report on Website | . | Data Cleaning | Clean the data appropriately. For instance, you may need to replace data that should be missing with NaN or create new columns out of given ones (e.g. compute distances, scale data, or get time information from time stamps). | Describe, in detail, the data cleaning steps you took and how they affected your analyses. The steps should be explained in reference to the data generating process. Show the head of your cleaned DataFrame (see Part 2: Report for instructions). | . | Univariate Analysis | Look at the distributions of relevant columns separately by using DataFrame operations and drawing at least two relevant plots. | Embed at least one plotly plot you created in your notebook that displays the distribution of a single column (see Part 2: Report for instructions). Include a 1-2 sentence explanation about your plot, making sure to describe and interpret any trends present. (Your notebook will likely have more visualizations than your website, and thatâ€™s fine. Feel free to embed more than one univariate visualization in your website if youâ€™d like, but make sure that each embedded plot is accompanied by a description.) | . | Bivariate Analysis | Look at the statistics of pairs of columns to identify possible associations. For instance, you may create scatter plots and plot conditional distributions, or box-plots. You must plot at least two such plots in your notebook. The results of your bivariate analyses will be helpful in identifying interesting hypothesis tests! | Embed at least one plotly plot that displays the relationship between two columns. Include a 1-2 sentence explanation about your plot, making sure to describe and interpret any trends present. (Your notebook will likely have more visualizations than your website, and thatâ€™s fine. Feel free to embed more than one bivariate visualization in your website if youâ€™d like, but make sure that each embedded plot is accompanied by a description.) | . | Interesting Aggregates | Choose columns to group and pivot by and examine aggregate statistics. | Embed at least one grouped table or pivot table in your website and explain its significance. | . Step 3: Assessment of Missingness . | Step | Analysis in Notebook | Report on Website | . | NMAR Analysis | Recall, to determine whether data are likely NMAR, you must reason about the data generating process; you cannot conclude that data are likely NMAR solely by looking at your data. As such, thereâ€™s no code to write here (and hence, nothing to put in your notebook). | State whether you believe there is a column in your dataset that is NMAR. Explain your reasoning and any additional data you might want to obtain that could explain the missingness (thereby making it MAR). Make sure to explicitly use the term â€œNMAR.â€ | . | Missingness Dependency | Pick a column in the dataset with non-trivial missingness to analyze, and perform permutation tests to analyze the dependency of the missingness of this column on other columns.Specifically, find at least one other column that the missingness of your selected column does depend on, and at least one other column that the missingness of your selected column does not depend on.Tip: Make sure you know the difference between the different types of missingness before approaching that section. Many students in the past have lost credit for mistaking one type of missingness for another.Note that some datasets may have special requirements for this section; look at the â€œSpecial Considerationsâ€ section of your chosen dataset for more details. | Present and interpret the results of your missingness permutation tests with respect to your data and question. Embed a plotly plot related to your missingness exploration; ideas include:â€¢ The distribution of column \\(Y\\) when column \\(X\\) is missing and the distribution of column \\(Y\\) when column \\(X\\) is not missing, as was done in Lecture 8.â€¢ The empirical distribution of the test statistic used in one of your permutation tests, along with the observed statistic. | . Step 4: Hypothesis Testing . | Step | Analysis in Notebook | Report on Website | . | Hypothesis Testing | Clearly state a pair of hypotheses and perform a hypothesis test or permutation test that is not related to missingness. Feel free to use one of the example questions stated in the â€œExample Questions and Prediction Problemsâ€ section of your datasetâ€™s description page or pose a hypothesis test of your own. | Clearly state your null and alternative hypotheses, your choice of test statistic and significance level, the resulting \\(p\\)-value, and your conclusion. Justify why these choices are good choices for answering the question you are trying to answer.Optional: Embed a visualization related to your hypothesis test in your website.Tip: When making writing your conclusions to the statistical tests in this project, never use language that implies an absolute conclusion; since we are performing statistical tests and not randomized controlled trials, we cannot prove that either hypothesis is 100% true or false. | . In Steps 5-8, you will build a predictive model, based on the knowledge of your dataset you developed in Steps 1-4. Step 5: Framing a Prediction Problem . | Step | Analysis in Notebook | Report on Website | . | Problem Identification | Identify a prediction problem. Feel free to use one of the example prediction problems stated in the â€œExample Questions and Prediction Problemsâ€ section of your datasetâ€™s description page or pose a hypothesis test of your own. The prediction problem you come up with doesnâ€™t have to be related to the question you were answering in Steps 1-4, but ideally, your entire project has some sort of coherent theme. | Clearly state your prediction problem and type (classification or regression). If you are building a classifier, make sure to state whether you are performing binary classification or multiclass classification. Report the response variable (i.e. the variable you are predicting) and why you chose it, the metric you are using to evaluate your model and why you chose it over other suitable metrics (e.g. accuracy vs. F1-score). Note: Make sure to justify what information you would know at the â€œtime of predictionâ€ and to only train your model using those features. For instance, if we wanted to predict your final exam grade, we couldnâ€™t use your Final Project grade, because the project is only due after the final exam! Feel free to ask questions if youâ€™re not sure. | . Step 6: Baseline Model . | Step | Analysis in Notebook | Report on Website | . | Baseline Model | Train a â€œbaseline modelâ€ for your prediction task that uses at least two features. (For this requirement, two features means selecting at least two columns from your original dataset that you should transform). You can leave numerical features as-is, but youâ€™ll need to take care of categorical columns using an appropriate encoding. Implement all steps (feature transforms and model training) in a single sklearn Pipeline. Note: Both now and in Step 7: Final Model, make sure to evaluate your modelâ€™s ability to generalize to unseen data! There is no â€œrequiredâ€ performance metric that your baseline model needs to achieve. | Describe your model and state the features in your model, including how many are quantitative, ordinal, and nominal, and how you performed any necessary encodings. Report the performance of your model and whether or not you believe your current model is â€œgoodâ€ and why.Tip: Make sure to hit all of the points above: many projects in the past have lost points for not doing so. | . Step 7: Final Model . | Step | Analysis in Notebook | Report on Website | . | Final Model | Create a â€œfinalâ€ model that improves upon the â€œbaselineâ€ model you created in Step 2. Do so by engineering at least two new features from the data, on top of any categorical encodings you performed in Baseline Model Step. (For instance, you may use a StandardScaler on a quantitative column and a QuantileTransformer transformer on a different column to get two new features.) Again, implement all steps in a single sklearn Pipeline. While deciding what features to use, you must perform a search for the best hyperparameters (e.g. tree depth) to use amongst a list(s) of options, either by using GridSearchCV or through some manual iterative method. In your notebook, state which hyperparameters you plan to tune and why before actually tuning them.Optional: You are encouraged to try many different modeling algorithms for your final model (i.e. LinearRegression, RandomForestClassifier, Lasso, SVC, etc.) If you do this, make sure to clearly indicate in your notebook which model is your actual final model as that will be used to grade the above requirements.Note 1: When training your model, make sure you use the same unseen and seen datasets from your baseline model. This way, the evaluation metric you get on your final model can be compared to your baselineâ€™s on the basis of the model itself and not the dataset it was trained on. Based on which method you use for hyperparameter tuning, this may mean that you will need to use some of your training data as your validation data. If this is the case, make sure to train your final model on the whole dataset prior to evaluation.Note 2: You will not be graded on â€œhow muchâ€ your model improved from Step 6: Baseline Model to Step 7: Final Model. What you will be graded on is on whether or not your model improved, as well as your thoughtfulness and effort in creating features, along with the other points above.Note 3: Donâ€™t try to improve your modelâ€™s performance just by blindly transforming existing features into new ones. Think critically about what each transformation youâ€™re doing actually does. For example, thereâ€™s no use in using a StandardScaler transformer if your goal is to reduce the RMSE of a linear model: as we learned in DSC 40A, and in Lecture 15, standardizing features in a regression model does not change the modelâ€™s predictions, only its coefficients! | State the features you added and why they are good for the data and prediction task. Note that you canâ€™t simply state â€œthese features improved my accuracyâ€, since youâ€™d need to choose these features and fit a model before noticing that â€“ instead, talk about why you believe these features improved your modelâ€™s performance from the perspective of the data generating process. Describe the modeling algorithm you chose, the hyperparameters that ended up performing the best, and the method you used to select hyperparameters and your overall model. Describe how your Final Modelâ€™s performance is an improvement over your Baseline Modelâ€™s performance.Optional: Include a visualization that describes your modelâ€™s performance, e.g. a confusion matrix, if applicable. | . Step 8: Fairness Analysis . | Step | Analysis in Notebook | Report on Website | . | Fairness Analysis | Perform a â€œfairness analysisâ€ of your Final Model from the previous step. That is, try and answer the question â€œdoes my model perform worse for individuals in Group X than it does for individuals in Group Y?â€, for an interesting choice of X and Y.As always, when comparing some quantitative attribute (in this case, something like precision or RMSE) across two groups, we use a permutation test. Letâ€™s illustrate how this works with an example. Letâ€™s suppose we have a sample voter dataset with columns 'Name', 'Age', and 'Voted', among others. We build a classifier that predicts whether someone voted (1) or didnâ€™t (0).Here, weâ€™ll say our two groups are â€¢ â€œyoung peopleâ€, people younger than 40â€¢ â€œold peopleâ€, people older than 40Note that in this example, we manually created these groups by binarizing the 'Age' column in our dataset, and thatâ€™s fine. (Remember, the Binarizer transformer with a threshold of 40 can do this for us.)For our evaluation metric, weâ€™ll choose precision. (In Week 10â€™s lectures, weâ€™ll look at other evaluation metrics and related parity measures for classifiers; choose the one that is most appropriate to your prediction task. If you built a regression model, you cannot use classification metrics like precision or recall; instead, you must use RMSE or \\(R^2\\).)Now, we must perform a permutation test. Before doing so, we must clearly state a null and an alternative hypothesis.â€¢ Null Hypothesis: Our model is fair. Its precision for young people and old people are roughly the same, and any differences are due to random chance.â€¢ Alternative Hypothesis: Our model is unfair. Its precision for young people is lower than its precision for old people.From here, you should be able to implement the necessary permutation test. The only other guidance we will provide you with is that you should not be modifying your model to produce different results when computing test statistics; use only your final fitted model from Final Model Step. | Clearly state your choice of Group X and Group Y, your evaluation metric, your null and alternative hypotheses, your choice of test statistic and significance level, the resulting \\(p\\)-value, and your conclusion.Optional: Embed a visualization related to your permutation test in your website. | . Style . While your website will neatly organized and tailored for public consumption, it is important to keep your analysis notebook organized as well. Follow these guidelines: . | Your work for each of the eight project steps described above (Introduction, Data Cleaning and Exploratory Data Analysis, â€¦, Fairness Analysis) should be completed in code cells underneath the Markdown header of that sectionâ€™s name. | You should only include work that is relevant to posing, explaining, and answering the question(s) you state in your website. You should include data quality, cleaning, and missingness assessments, and intermediate models and features you tried, though these should broadly be relevant to the tasks at hand. | Make sure to clearly explain what each component of your notebook means. Specifically: . | All plots should have titles, labels, and a legend (if applicable), even if they donâ€™t make it into your website. Plots should be self-contained â€“ readers should be able to understand what they describe without having to read anything else. | All code cells should contain a comment describing how the code works (unless the code is self-explanatory â€“ use your best judgement). | . | . ",
    "url": "/proj04/#part-1-analysis",
    "relUrl": "/proj04/#part-1-analysis"
  },"5": {
    "doc": "ğŸ“Š Final Project",
    "title": "Part 2: Report",
    "content": "The purpose of your website is to provide the general public â€“ your classmates, friends, family, recruiters, and random internet strangers â€“ with an overview of your project and its findings, without forcing them to understand every last detail. We donâ€™t expect the website creation process to take very much time, but it will certainly be rewarding. Once youâ€™ve completed your analysis and know what you will put in your website, start reading this section. Your website must clearly contain the following eight headings, corresponding to the eight steps mentioned in Part 1: . | Introduction | Data Cleaning and Exploratory Data Analysis | Assessment of Missingness | Hypothesis Testing | Framing a Prediction Problem | Baseline Model | Final Model | Fairness Analysis | . Donâ€™t include â€œStep 1â€, â€œStep 2â€, etc. in your headings â€“ the headings should be exactly as they are above. The specific content your website needs to contain is described in the â€œReport on Websiteâ€ columns above. Make sure to also give your website a creative title that relates to the question youâ€™re trying to answer, and clearly state your name(s). Your report will be in the form of a static website, hosted for free on GitHub Pages. More specifically, youâ€™ll use Jekyll, a framework built into GitHub Pages that allows you to create professional-looking websites just by writing Markdown (dsc80.com is built using Jekyll!). GitHub Pages does the â€œhardâ€ part of converting your Markdown to HTML. If youâ€™d like to follow the official GitHub Pages &amp; Jekyll tutorial, youâ€™re welcome to, though we will provide you with a perhaps simpler set of instructions here. A very basic site with the required headings and one embedded visualization can be found at rampure.org/dsc80-proj3-test1/; the source code for the site is here. Step 1: Initializing a Jekyll GitHub Pages Site . | Create a GitHub account, if you donâ€™t already have one. | Create a new GitHub repository for your project. | GitHub Pages sites live at &lt;username&gt;.github.io/&lt;reponame&gt; (for instance, the site for github.com/dsc-courses/dsc80-2025-wi is dsc-courses.github.io/dsc80-2025-wi). | As such, donâ€™t include â€œDSC 80â€ or â€œFinal Projectâ€ in your repoâ€™s name â€“ this looks unprofessional to future employers, and gives you a generic-sounding URL. Instead, mention that this is a project for DSC 80 at UCSD in the repository description. | Make sure to make your repository public. | Select â€œADD a README file.â€ This ensures that your repository starts off non-empty, which is necessary to continue. | . | Click â€œSettingsâ€ in the repository toolbar (next to â€œInsightsâ€), then click â€œPagesâ€ in the left menu. | Under â€œBranchâ€, click the â€œNoneâ€ dropdown, change the branch to â€œmainâ€, and then click â€œSave.â€ You should soon see â€œGitHub Pages source saved.â€ in a blue banner at the top of the page. This initiates GitHub Pages in your repository. | After 30 seconds, reload the page again. You should see â€œYour site is live at http://username.github.io/reponame/â€. Click that link â€“ you now have a site! | Click â€œCodeâ€ in the repo toolbar to return to the source code for your repo. | . Note that the source code for your site lives in README.md. As you push changes to README.md, they will update on your site automatically within a few minutes! Before moving forward, make sure that you can make basic edits: . | Clone your repo locally. | Make some edits to README.md. | Push those changes back to GitHub, using the following steps: . | Add your changes to â€œstagingâ€ using git add README.md (repeat this for any other files you add). | Commit your changes using git commit -m '&lt;some message here&gt;', e.g. git commit -m 'changed title of website'. | Push your changes using git push. | . | . Moving forward, we recommend making edits to your website source code locally, rather than directly on GitHub. This is in part due to the fact that youâ€™ll be creating folders and adding files to your source code. Step 2: Choosing a Theme . The default â€œthemeâ€ of a Jekyll site is not all that interesting. To change the theme of your site: . | Create a file in your repository called _config.yml. | Go here, and click the links of various themes until you find one you like. | Open the linked repository for the theme youâ€™d like to use and scroll down to the â€œUsageâ€ section of the README. Copy the necessary information from the README to your _config.yml and push it to your site. | . For instance, if I wanted to use the Merlot theme, Iâ€™d put the following in my _config.yml: . remote_theme: pages-themes/merlot@v0.2.0 plugins: - jekyll-remote-theme # add this line to the plugins list if you already have one . Note that youâ€™re free to use any Jekyll theme, not just the ones that appear here. You are required to choose some theme other than the default, though. See more details about themes here. Step 3: Embedding Content . Now comes the interesting part â€“ actually including content in your site. The Markdown cheat sheet contains tips on how to format text and other page components in Markdown (and if youâ€™d benefit by seeing an example, you could always look at the Markdown source of this very page â€“ meta!). What will be a bit trickier is embedding plotly plots in your site so that they are interactive. Note that you are required to do this, you cannot simply take screenshots of plots from your notebooks and embed them in your site. Hereâ€™s how to embed a plotly plot directly in your site. | First, youâ€™ll need to convert your plot to HTML. If fig is a plotly Figure object (for instance, the result of calling px.express, go.Figure, or .plot when pd.options.plotting.backend = \"plotly\" has been run), then the method fig.write_html saves the plot as HTML to a file. Call it using fig.write_html('file-name.html', include_plotlyjs='cdn'). | Change 'file-name.html' to the path where youâ€™d like to initially save your plot. | include_plotlyjs='cdn' tells write_html to load the source code for the plotly library from a server, rather than including the entire source code in your HTML file. This drastically reduces the size of the resulting HTML file, keeping your repo size down. | . | Move the .html file(s) youâ€™ve created into a folder in your website repo called assets (or something similar). | Depending on where your template notebook is saved, you could combine this step with the step above by calling fig.write_html with the correct path (e.g. `fig.write_html(â€˜../league-match-analysis/assets/matches-per-year.htmlâ€™)). | . | In README.md, embed your plot using the following syntax: | . &lt;iframe src=\"assets/file-name.html\" width=\"800\" height=\"600\" frameborder=\"0\" &gt;&lt;/iframe&gt; . | iframe stands for â€œinline frameâ€; it allows us to embed HTML files within other HTML files. | You can change the width and height arguments, but donâ€™t change the frameBorder argument. | . Refer here for a working example. Try your best to make your plots look professional and unique to your group â€“ add axis labels, change the font and colors, add annotations, etc. Remember, potential employers will see this â€“ you donâ€™t want your plots to look like everyone elseâ€™s! If youâ€™d like to match the styles of the plotly plots used in lecture (e.g. with the white backgrounds), you can import and use the dsc80_utils.py thatâ€™s in the proj04 folder of our public repo, alongside template.ipynb. To convert a DataFrame in your notebook to Markdown source code (which you need to do for both the Data Cleaning and Interesting Aggregates sections of Step 2: Data Cleaning and Exploratory Data Analysis in Part 1), use the .to_markdown() method on the DataFrame. For instance, . print(counts[['Quarter', 'Count']].head().to_markdown(index=False)) . displays a string, containing the Markdown representation of the first 5 rows of counts, including only the 'Quarter' and 'Count' columns (and not including the index). You can see how this appears here; remember, no screenshots. You may need to play with this a little bit so that you donâ€™t show DataFrames that are super, super wide and look ugly. Local Setup . The above instructions give you all you need to create and make updates to your site. However, you may want to set up Jekyll locally, so that you can look at how changes to your site would look without having to push and wait for GitHub to re-build your site. To do so, follow the steps here and then here. ",
    "url": "/proj04/#part-2-report",
    "relUrl": "/proj04/#part-2-report"
  },"6": {
    "doc": "ğŸ“Š Final Project",
    "title": "Submission and Rubric",
    "content": "As mentioned at the top of this page, this project has two checkpoints, each worth 1% of your overall grade. Checkpoint 1 Submission . The first checkpoint is due on Friday, February 28th. You can submit the checkpoint here on Gradescope. The checkpoint 1 assignment is worth 20 points, and asks you to answer the following questions: . | (2 points) Which of the three datasets did you choose? Why? | (6 points) Upload a screenshot of a plotly visualization youâ€™ve created while completing Part 1, Step 2: Data Cleaning and Exploratory Data Analysis. | (6 points) What is the pair of hypotheses you plan on testing in Part 1, Step 4? What is the test statistic you plan on using? | (6 points) What is the column you plan on trying to predict in Part 1, Steps 5-8? Is it a classification or regression problem? | . Checkpoint 2 Submission . The second one is due on Friday, March 7th. You can submit the checkpoint here on Gradescope. The checkpoint 2 assignment is worth 20 points, and asks you to answer the following questions: . | (7.5 points) For step 4, what are your null and alternative hypotheses, and what were the result? | (7.5 points) Briefly explain your baseline model and your plans for improving the model. | (5 points) Submit a working GitHub page webpage URL for the project. On the webpage, you need to at least include a project title. | . Final Submission . You will ultimately submit your project in two ways: . | By uploading a PDF version of your notebook to the specific â€œFinal Project Notebook PDF (Dataset)â€ assignment on Gradescope for your dataset. | To export your notebook as a PDF, first, restart your kernel and run all cells. Then, go to â€œFile &gt; Print Previewâ€. Then, save a print preview of the webpage as a PDF. There are other ways to save a notebook as a PDF but they may require that you have additional packages installed on your computer, so this is likely the most straightforward. | Itâ€™s fine if your plotly graphs donâ€™t render in the PDF output of your notebook. However, make sure none of the code is cut off in your notebookâ€™s PDF. You will lose 5% of the points available on this project if your code is cut off. | This notebook asks you to include a link to your website; make sure to do so. | . | By submitting a link to your website to the â€œFinal Project Website Link (All Datasets)â€ assignment on Gradescope. | . To both submissions, make sure to tag your partner. You donâ€™t need to submit your actual .ipynb file anywhere. While your website must be public and you should share it with others, you should not make your code for this project available publicly. Since there are two assignments you need to submit on Gradescope, we will treat your submission time as being the latter of your two submissions. So, if you submit to the â€œFinal Project Notebook PDFâ€ assignment before the deadline but to the â€œFinal Project Website Link (All Datasets)â€ website one day late, overall, that counts as late submission. There are a lot of moving parts to this assignment â€“ donâ€™t wait until the last minute to try and submit! . Rubric . Your project will be graded out of 200 points. The rough rubric is shown below. If you satisfy these requirements as described above, you will receive full credit. Note that the rubric is intentionally vague when it comes to Steps 5-8. This is because an exact rubric would specify exactly what you need to do to build a model, but figuring out what to do is a large part of Steps 5-8. | Component | Weight | . | Step 1: Introduction | 8 points | . | Step 2: Data Cleaning and Exploratory Data Analysis â€¢ Cleaned data (8 points)â€¢ Performed univariate analyses (8 points)â€¢ Performed bivariate analyses and aggregations (8 points) | 24 points | . | Step 3: Assessment of Missingness â€¢ Addressed NMAR question (4 points) â€¢ Performed permutation tests for missingness (8 points) â€¢ Interpreted missingness test results (8 points) | 20 points | . | Step 4: Hypothesis Testingâ€¢ Selected relevant columns for a hypothesis or permutation test (4 points)â€¢ Explicitly stated a null hypothesis (4 points)â€¢ Explicitly stated an alternative hypothesis (4 points)â€¢ Performed a hypothesis or permutation test (8 points)â€¢ Used a valid test statistic (4 points)â€¢ Computed a \\(p\\)-value and made a decision (4 points) | 28 points | . | Step 5: Framing a Prediction Problem see requirements above | 15 points | . | Step 6: Baseline Model see requirements above | 35 points | . | Step 7: Final Model see requirements above | 35 points | . | Step 8: Fairness Analysis see requirements above | 15 points | . | Overall: Included all necessary components on the website | 20 points | . | Total | 200 points | . ",
    "url": "/proj04/#submission-and-rubric",
    "relUrl": "/proj04/#submission-and-rubric"
  },"7": {
    "doc": "ğŸ“Š Final Project",
    "title": "ğŸ“Š Final Project",
    "content": " ",
    "url": "/proj04/",
    "relUrl": "/proj04/"
  },"8": {
    "doc": "ğŸ  Home",
    "title": "Practice and Application of Data Science",
    "content": "DSC 80, Winter 2025 at UC San Diego . Duncan Watson-Parris he/him . dwatsonparris@ucsd.edu Lecture(s): TuTh 11:00AM-12:20PM in SOLIS 104. Podcasts Welcome Survey Extension Request Form . Week 1 â€“ Dataframes . Tue Jan 7 LEC 1 . Introduction, Data Science Lifecycle . ğŸŒ— blankğŸ“ filled Ch. 1 . Thu Jan 9 LEC 2 . DataFrame Fundamentals . ğŸŒ— blankğŸ“ filled Ch. 6, 6.1 . Week 2 â€“ Dataframes . Tue Jan 14 LEC 3 . Aggregating . ğŸŒ— blankğŸ“ filled Ch. 6.2 . Wed Jan 15 LAB 1 . Python, NumPy, and Pandas . Thu Jan 16 LEC 4 . Simpson's Paradox, Joining, Transforming . ğŸŒ— blankğŸ“ filled Ch. 6.3-6.5 . Fri Jan 17 PROJ 1 . Project 1 checkpoint . Week 3 â€“ Messy Data, Statistical Testing . Tue Jan 21 LEC 5 . Exploring and Cleaning Data . ğŸŒ— blankğŸ“ filled Ch. 9 and 10 . Wed Jan 22 LAB 2 . More Pandas . Thu Jan 23 LEC 6 . Hypothesis and Permutation Testing . ğŸŒ— blankğŸ“ filled Ch. 17 . Fri Jan 24 PROJ 1 . Project 1 . Week 4 â€“ Missing Values . Tue Jan 28 LEC 7 . Missingness Mechanisms . ğŸ“ filled A1, A2 . Wed Jan 29 LAB 3 . DataFrame Manipulation . Thu Jan 30 LEC 8 . Imputation . ğŸ“ filled DSP 6.3-6.5 . Fri Jan 31 PROJ 2 . Project 2 checkpoint . Week 5 â€“ HTTP . Tue Feb 4 LEC 9 . HTTP Basics . ğŸ“ filled Ch. 14.2-14.4 . Wed Feb 5 LAB 4 . Hypothesis and Permutation Testing . Ch. 17 . Thu Feb 6 LEC 10 . Web Scraping . ğŸ“ filled Ch. 14.2-14.4 . Fri Feb 7 PROJ 2 . Project 2 . Week 6 â€“ Web data . Mon Feb 10 DISC . Midterm Review . ğŸŒ— blankğŸ“ filled Zoom Recording password: v?V22U$% . Tue Feb 11 EXAM . Midterm Exam . info sheet . Wed Feb 12 LAB 5 . Missing Values and Imputation . Thu Feb 13 LEC 11 . Regular Expressions . ğŸ“ filled Ch. 13 . Fri Feb 14 PROJ 3 . Project 3 checkpoint . Week 7 â€“ Text data, Modeling . Tue Feb 18 LEC 12 . Text Features . ğŸ“ filled Ch. 13.4 . Wed Feb 19 LAB 6 . HTTP and HTML . Thu Feb 20 LEC 13 . Linear Regression . ğŸ“ filled Ch. 15.0-15.6 . Fri Feb 21 PROJ 3 . Project 3 . Week 8 â€“ Feature Engineering . Tue Feb 25 LEC 14 . Feature Engineering . ğŸ“ filled Ch. 15.7-15.9 . Wed Feb 26 LAB 7 . Regular Expressions and Text Data . Thu Feb 27 LEC 15 . Standardization, Multicollinearity, and Generalization . ğŸ“ filled Ch. 16, 17.6 . Fri Feb 28 FINAL PROJ . Final Project Checkpoint 1 . Week 9 â€“ Modeling in Practice . Tue Mar 4 LEC 16 . Hyperparameters, Cross-Validation, and Decision Trees . ğŸ“ filled Ch. 16 . Wed Mar 5 LAB 8 . Modeling and Feature Engineering . Thu Mar 6 LEC 17 . Decision Trees and Random Forests . ğŸ“ filled A1 . Fri Mar 7 FINAL PROJ . Final Project Checkpoint 2 . Week 10 â€“ Evaluating Classifiers . Mon Mar 10 DISC . Final Review (7-9pm) . ğŸŒ— blankğŸ“ filled Zoom Recording . Tue Mar 11 LEC 18 . Classifier Evaluation, Model Fairness and Conclusion . ğŸ“ filled Ch. 16, Ch. 19.5 . Fri Mar 14 FINAL PROJ . Final Project . Week 11 â€“ Final Exam . Thu Mar 20 EXAM . Final Exam . ",
    "url": "/#practice-and-application-of-data-science",
    "relUrl": "/#practice-and-application-of-data-science"
  },"9": {
    "doc": "ğŸ  Home",
    "title": "ğŸ  Home",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"10": {
    "doc": "League of Legends âŒ¨ï¸",
    "title": "League of Legends âŒ¨ï¸",
    "content": " ",
    "url": "/proj04/league-of-legends/",
    "relUrl": "/proj04/league-of-legends/"
  },"11": {
    "doc": "League of Legends âŒ¨ï¸",
    "title": "Table of Contents",
    "content": ". | Getting the Data | Example Questions and Prediction Problems | Special Considerations . | Step 2: Data Cleaning and Exploratory Data Analysis | . | . Welcome to Summonerâ€™s Rift! This dataset contains information of players and teams from over 10,000 League of Legends competitive matches. Youâ€™ll probably want to be at least a little bit familiar with League of Legends and its terminology to use this dataset. If not, one of the other datasets may be more interesting to you. ",
    "url": "/proj04/league-of-legends/#table-of-contents",
    "relUrl": "/proj04/league-of-legends/#table-of-contents"
  },"12": {
    "doc": "League of Legends âŒ¨ï¸",
    "title": "Getting the Data",
    "content": "The data can be found on the website Oracleâ€™s Elixir at the provided Google Drive link. Weâ€™ve verified that itâ€™s possible to satisfy the requirements of the project using match data from 2022. Youâ€™re welcome to use newer or older datasets if you wish, but keep in mind that League of Legends changes significantly between years; this can make it difficult to combine or make comparisons between datasets from different years. ",
    "url": "/proj04/league-of-legends/#getting-the-data",
    "relUrl": "/proj04/league-of-legends/#getting-the-data"
  },"13": {
    "doc": "League of Legends âŒ¨ï¸",
    "title": "Example Questions and Prediction Problems",
    "content": "Feel free to base your exploration into the dataset in Steps 1-4 around one of these questions, or come up with a question of your own. | Looking at tier-one professional leagues, which league has the most â€œaction-packedâ€ games? Is the amount of â€œactionâ€ in this league significantly different than in other leagues? Note that youâ€™ll have to come up with a way of quantifying â€œactionâ€. | Which competitive region has the highest win rate against teams outside their region? Note you will have to find and merge region data for this question as the dataset does not have it. | Which role â€œcarriesâ€ (does the best) in their team more often: ADCs (Bot lanes) or Mid laners? | Is Talon (former tutor Costinâ€™s favorite champion) more likely to win or lose any given match? | . Feel free to use one of the prompts below to build your predictive model in Steps 5-8, or come up with a prediction task of your own. | Predict if a team will win or lose a game. | Predict which role (top-lane, jungle, support, etc.) a player played given their post-game data. | Predict how long a game will take before it happens. | Predict which team will get the first Baron. | . Make sure to justify what information you would know at the â€œtime of predictionâ€ and to only train your model using those features. ",
    "url": "/proj04/league-of-legends/#example-questions-and-prediction-problems",
    "relUrl": "/proj04/league-of-legends/#example-questions-and-prediction-problems"
  },"14": {
    "doc": "League of Legends âŒ¨ï¸",
    "title": "Special Considerations",
    "content": "Step 2: Data Cleaning and Exploratory Data Analysis . | Each 'gameid' corresponds to up to 12 rows â€“ one for each of the 5 players on both teams and 2 containing summary data for the two teams (try to find out what distinguishes those rows). After selecting your line of inquiry, make sure to remove either the player rows or the team rows so as not to have issues later in your analysis. | Many columns should be of type bool but are not. | . ",
    "url": "/proj04/league-of-legends/#special-considerations",
    "relUrl": "/proj04/league-of-legends/#special-considerations"
  },"15": {
    "doc": "Power Outages ğŸ”‹",
    "title": "Power Outages ğŸ”‹",
    "content": " ",
    "url": "/proj04/power-outages/",
    "relUrl": "/proj04/power-outages/"
  },"16": {
    "doc": "Power Outages ğŸ”‹",
    "title": "Table of Contents",
    "content": ". | Getting the Data | Example Questions and Prediction Problems | Special Considerations . | Step 2: Data Cleaning and Exploratory Data Analysis | . | . This dataset has major power outage data in the continental U.S. from January 2000 to July 2016. ",
    "url": "/proj04/power-outages/#table-of-contents",
    "relUrl": "/proj04/power-outages/#table-of-contents"
  },"17": {
    "doc": "Power Outages ğŸ”‹",
    "title": "Getting the Data",
    "content": "The data is downloadable here. Note: If you are having a hard time with the â€œThis datasetâ€ link, hold shift and click the link to open it into a new tab and then refresh that new tab. A data dictionary is available at this article under Table 1. Variable descriptions. ",
    "url": "/proj04/power-outages/#getting-the-data",
    "relUrl": "/proj04/power-outages/#getting-the-data"
  },"18": {
    "doc": "Power Outages ğŸ”‹",
    "title": "Example Questions and Prediction Problems",
    "content": "Feel free to base your exploration into the dataset in Steps 1-4 around one of these questions, or come up with a question of your own. | Where and when do major power outages tend to occur? | What are the characteristics of major power outages with higher severity? Variables to consider include location, time, climate, land-use characteristics, electricity consumption patterns, economic characteristics, etc. What risk factors may an energy company want to look into when predicting the location and severity of its next major power outage? | What characteristics are associated with each category of cause? | How have characteristics of major power outages changed over time? Is there a clear trend? | . Feel free to use one of the prompts below to build your predictive model in Steps 5-8, or come up with a prediction task of your own. | Predict the severity (in terms of number of customers, duration, or demand loss) of a major power outage. | Predict the cause of a major power outage. | Predict the number and/or severity of major power outages in the year 2022. | Predict the electricity consumption of an area. | . Make sure to justify what information you would know at the â€œtime of predictionâ€ and to only train your model using those features. ",
    "url": "/proj04/power-outages/#example-questions-and-prediction-problems",
    "relUrl": "/proj04/power-outages/#example-questions-and-prediction-problems"
  },"19": {
    "doc": "Power Outages ğŸ”‹",
    "title": "Special Considerations",
    "content": "Step 2: Data Cleaning and Exploratory Data Analysis . | The data is given as an Excel file rather than a CSV. Open the data in Google Sheets or another spreadsheet application and determine which rows and columns of the sheet should be ignored when loading the data in pandas. | Note that pandas can load multiple filetypes: pd.read_csv, pd.read_excel, pd.read_html, pd.read_json, etc. | . | The power outage start date and time is given by 'OUTAGE.START.DATE' and 'OUTAGE.START.TIME'. It would be preferable if these two columns were combined into one pd.Timestamp column. Combine 'OUTAGE.START.DATE' and 'OUTAGE.START.TIME' into a new pd.Timestamp column called 'OUTAGE.START'. Similarly, combine 'OUTAGE.RESTORATION.DATE' and 'OUTAGE.RESTORATION.TIME' into a new pd.Timestamp column called 'OUTAGE.RESTORATION'. | pd.to_datetime and pd.to_timedelta will be useful here. | . | To visualize geospatial data, consider Folium or another geospatial plotting library. You can even embed Folium maps in your website! If fig is a folium.folium.Map object, then fig._repr_html_() evaluates to a string containing your plot as HTML; use open and write to write this string to an .html file. | . ",
    "url": "/proj04/power-outages/#special-considerations",
    "relUrl": "/proj04/power-outages/#special-considerations"
  },"20": {
    "doc": "Recipes and Ratings ğŸ½ï¸",
    "title": "Recipes and Ratings ğŸ½ï¸",
    "content": " ",
    "url": "/proj04/recipes-and-ratings/",
    "relUrl": "/proj04/recipes-and-ratings/"
  },"21": {
    "doc": "Recipes and Ratings ğŸ½ï¸",
    "title": "Table of Contents",
    "content": ". | Getting the Data . | Recipes | Ratings | . | Example Questions and Prediction Problems | Special Considerations . | Step 2: Data Cleaning and Exploratory Data Analysis | Step 4: Assessment of Missingness | . | . This dataset contains recipes and ratings from food.com. It was originally scraped and used by the authors of this recommender systems paper. ",
    "url": "/proj04/recipes-and-ratings/#table-of-contents",
    "relUrl": "/proj04/recipes-and-ratings/#table-of-contents"
  },"22": {
    "doc": "Recipes and Ratings ğŸ½ï¸",
    "title": "Getting the Data",
    "content": "Download the data here. Youâ€™ll download two CSV files: . | RAW_recipes.csv contains recipes. | RAW_interactions.csv contains reviews and ratings submitted for the recipes in RAW_recipes.csv. | . Weâ€™ve provided you with a subset of the raw data used in the original report, containing only the recipes and reviews posted since 2008, since the original data is quite large. A description of each column in both datasets is given below. Recipes . For context, you may want to look at an example recipe directly on food.com. | Column | Description | . | 'name' | Recipe name | . | 'id' | Recipe ID | . | 'minutes' | Minutes to prepare recipe | . | 'contributor_id' | User ID who submitted this recipe | . | 'submitted' | Date recipe was submitted | . | 'tags' | Food.com tags for recipe | . | 'nutrition' | Nutrition information in the form [calories (#), total fat (PDV), sugar (PDV), sodium (PDV), protein (PDV), saturated fat (PDV), carbohydrates (PDV)]; PDV stands for â€œpercentage of daily valueâ€ | . | 'n_steps' | Number of steps in recipe | . | 'steps' | Text for recipe steps, in order | . | 'description' | User-provided description | . Ratings . | Column | Description | . | 'user_id' | User ID | . | 'recipe_id' | Recipe ID | . | 'date' | Date of interaction | . | 'rating' | Rating given | . | 'review' | Review text | . After downloading the datasets, you must follow the following steps to merge the two datasets and create a column containing the average rating per recipe: . | Left merge the recipes and interactions datasets together. | In the merged dataset, fill all ratings of 0 with np.nan. (Think about why this is a reasonable step, and include your justification in your website.) | Find the average rating per recipe, as a Series. | Add this Series containing the average rating per recipe back to the recipes dataset however youâ€™d like (e.g., by merging). Use the resulting dataset for all of your analysis. (For the purposes of Project 4, the 'review' column in the interactions dataset doesnâ€™t have much use.) | . ",
    "url": "/proj04/recipes-and-ratings/#getting-the-data",
    "relUrl": "/proj04/recipes-and-ratings/#getting-the-data"
  },"23": {
    "doc": "Recipes and Ratings ğŸ½ï¸",
    "title": "Example Questions and Prediction Problems",
    "content": "Feel free to base your exploration into the dataset in Steps 1-4 around one of these questions, or come up with a question of your own. | What types of recipes tend to have the most calories? | What types of recipes tend to have higher average ratings? | What types of recipes tend to be healthier (i.e. more protein, fewer carbs)? | What is the relationship between the cooking time and average rating of recipes? | . Feel free to use one of the prompts below to build your predictive model in Steps 5-8, or come up with a prediction task of your own. | Predict ratings of recipes. | Predict the number of minutes to prepare recipes. | Predict the number of steps in recipes. | Predict calories of recipes. | . Make sure to justify what information you would know at the â€œtime of predictionâ€ and to only train your model using those features. ",
    "url": "/proj04/recipes-and-ratings/#example-questions-and-prediction-problems",
    "relUrl": "/proj04/recipes-and-ratings/#example-questions-and-prediction-problems"
  },"24": {
    "doc": "Recipes and Ratings ğŸ½ï¸",
    "title": "Special Considerations",
    "content": "Step 2: Data Cleaning and Exploratory Data Analysis . Some columns, like 'nutrition', contain values that look like lists, but are actually strings that look like lists. You may want to turn the strings into actual lists, or create columns for every unique value in those lists. For instance, per the data dictionary, each value in the 'nutrition' column contains information in the form \"[calories (#), total fat (PDV), sugar (PDV), sodium (PDV), protein (PDV), saturated fat (PDV), and carbohydrates (PDV)]\"; you could create individual columns in your dataset titled 'calories', 'total fat', etc. Step 4: Assessment of Missingness . There are only three columns in the merged dataset that contain missing values; make sure youâ€™re using the merged dataset for all of your analysis (and that you followed the steps at the top of this page exactly). ",
    "url": "/proj04/recipes-and-ratings/#special-considerations",
    "relUrl": "/proj04/recipes-and-ratings/#special-considerations"
  },"25": {
    "doc": "ğŸ“š Resources",
    "title": "ğŸ“š Resources",
    "content": " ",
    "url": "/resources/",
    "relUrl": "/resources/"
  },"26": {
    "doc": "ğŸ“š Resources",
    "title": "Table of contents",
    "content": ". | Past Exams | Videos | Readings . | Textbooks | Lecture-Specific Readings | Articles | Other Links | . | Regular Expressions | . ",
    "url": "/resources/#table-of-contents",
    "relUrl": "/resources/#table-of-contents"
  },"27": {
    "doc": "ğŸ“š Resources",
    "title": "Past Exams",
    "content": "Past exams with detailed solutions can be found at practice.dsc80.com. Weâ€™re constantly working to add more exams and solutions to the site. ",
    "url": "/resources/#past-exams",
    "relUrl": "/resources/#past-exams"
  },"28": {
    "doc": "ğŸ“š Resources",
    "title": "Videos",
    "content": ". | DSC 80 Environment Setup and Assignment Workflow. | Working with the command-line in DSC 80. | . ",
    "url": "/resources/#videos",
    "relUrl": "/resources/#videos"
  },"29": {
    "doc": "ğŸ“š Resources",
    "title": "Readings",
    "content": "Textbooks . | Principles and Techniques of Data Science, the textbook for Berkeleyâ€™s Data 100 course. Most of our supplemental readings will come from here. | notes.dsc80.com. These notes were originally written for DSC 80 but have not been updated in a few years. Some of our supplemental readings will come from here. | Wes McKinney. â€œPython for Data Analysisâ€. | DSC 10 Course Notes â€“ great refresher on babypandas. | Computational and Inferential Thinking, the textbook for Berkeleyâ€™s Data 8 course. | . Lecture-Specific Readings . | Fast Permutation Tests. | More Missingness Examples. | . Articles . | Views and Copies in pandas â€“ a great read if youâ€™d like to learn more about the infamous SettingWithCopyWarning. | jwilber.me/permutationtest, a great visual explanation of permutation testing. | A Visual Introduction to Machine Learning and Model Tuning and the Bias-Variance Tradeoff, excellent visual descriptions of the last few weeks of the course (some terminology is different, but the ideas are the same). | New: MLU Explain, a collection of interactive articles (prepared by Jared Wilber) that explain core machine learning ideas, like cross-validation, random forests, and precision and recall. | . Other Links . | pandastutor.com, the equivalent of pythontutor.com for DataFrame manipulation. | . ",
    "url": "/resources/#readings",
    "relUrl": "/resources/#readings"
  },"30": {
    "doc": "ğŸ“š Resources",
    "title": "Regular Expressions",
    "content": ". | regex101.com, a helpful site to have open while writing regular expressions. | Python re library documentation and how-to. | regex â€œcheat sheetâ€ (taken from here). | . ",
    "url": "/resources/#regular-expressions",
    "relUrl": "/resources/#regular-expressions"
  },"31": {
    "doc": "ğŸ‘©â€ğŸ« Staff",
    "title": "ğŸ‘©â€ğŸ« Staff",
    "content": " ",
    "url": "/staff/",
    "relUrl": "/staff/"
  },"32": {
    "doc": "ğŸ‘©â€ğŸ« Staff",
    "title": "Instructor",
    "content": "Duncan Watson-Parris . dwatsonparris@ucsd.edu . Duncan Watson-Parris is an Assistant Professor in the HalÄ±cÄ±oÄŸlu Data Science Institute and Scripps Institution of Oceanography, working at the interface of climate research and machine learning. The Climate Analytics Lab he leads focuses on understanding the interactions between aerosols and clouds, and their representation within global climate models. CAL is leading the development of a variety of machine learning tools and techniques to optimally combine the huge variety of available observational datasets, including global satellite and aircraft measurements, to constrain and improve these important models. ",
    "url": "/staff/#instructor",
    "relUrl": "/staff/#instructor"
  },"33": {
    "doc": "ğŸ‘©â€ğŸ« Staff",
    "title": "Staff",
    "content": "TA Mizuho Fukuda she/her . mfukuda@ucsd.edu . ğŸ“ 1st year MS, Data Science . ğŸ  Tokyo, Japan . ğŸ™‹ knitting, crocheting, exploring san diego . ğŸ¥˜ costa brava, ironside fish &amp; oyster, cesarina, cafe madeleine . Tutor Gabriel Cha he/him . gcha@ucsd.edu . ğŸ“ 4th year, Data Science, Sixth . ğŸ  Houston, TX . ğŸ™‹ keyboards, internships, and chatGPT . ğŸ§‹ tacos el gordo, omomo tea, and blue bottle . Tutor Ziqi Shang he/him . zishang@ucsd.edu . ğŸ“ 4th year Applied Mathematics, Economics, Warren . ğŸ  Shandong, China . ğŸ™‹ golfâ›³ï¸, billiardsğŸ±, and carğŸï¸ . ğŸ² Yummy House, Village Kitchen, II Fornaio . Tutor Ylesia Wu she/her . xw001@ucsd.edu . ğŸ“ 4th year, Data Science, ERC . ğŸ  Beijing, China . ğŸ™‹ Chinese music, ice skating, electric unicycle . ğŸ¥® BenGongâ€™s Tea, The Noble Chef, Mo-Mo-Paradise . ",
    "url": "/staff/#staff",
    "relUrl": "/staff/#staff"
  },"34": {
    "doc": "ğŸ“– Syllabus",
    "title": "ğŸ“– Syllabus",
    "content": " ",
    "url": "/syllabus/#-syllabus",
    "relUrl": "/syllabus/#-syllabus"
  },"35": {
    "doc": "ğŸ“– Syllabus",
    "title": "Table of contents",
    "content": ". | About ğŸ§ | Getting Started ğŸ’» . | Websites | Development Environment | Forms | . | Communication ğŸ’¬ | Course Components ğŸ . | Lectures | Discussions | Labs | Projects | Office Hours | Weekly Schedule | . | Exams ğŸ“ . | Redemption Policy | . | Policies ğŸ’¯ . | Grading | Late Policy | Redemption Policy for Labs and Projects | Regrade Requests | Incompletes | A note on letter grades | . | Collaboration Policy and Academic Integrity ğŸ¤ . | Why is academic integrity important? | What counts as cheating? | Use of Generative Artificial Intelligence | . | Support ğŸ«‚ . | Accommodations | Diversity and Inclusion | Campus Resources . | Food Support for Students: | . | . | Acknowledgements ğŸ™ | . ",
    "url": "/syllabus/#table-of-contents",
    "relUrl": "/syllabus/#table-of-contents"
  },"36": {
    "doc": "ğŸ“– Syllabus",
    "title": "About ğŸ§",
    "content": "DSC 80 serves as a bridge between lower-division and upper-division data science courses. In DSC 80, students will gain proficiency with the data science life cycle and learn many of the fundamental principles and techniques of data science spanning algorithms, statistics, machine learning, visualization, and data systems. After DSC 80, students will be prepared for data science internships and interviews, will have the tools to create their own data science portfolios, and will have the maturity necessary to succeed in upper-division machine learning and statistics courses. Prerequisites: DSC 30 and DSC 40A. ",
    "url": "/syllabus/#about-",
    "relUrl": "/syllabus/#about-"
  },"37": {
    "doc": "ğŸ“– Syllabus",
    "title": "Getting Started ğŸ’»",
    "content": "The course website, dsc80.com, will contain links to all course content. There are also a few things youâ€™ll need to do to get set up. Websites . Youâ€™ll need to make accounts on the following sites: . | Ed: Weâ€™ll be using Ed as our course message and discussion board. More details are in the Communication section below. If you didnâ€™t already get an invitation to our Ed course, sign up here. | Gradescope: Youâ€™ll submit all assignments and exams to Gradescope. This is where all of your grades will live as well. Most of the assignments will be coding assignments. Parts of these assignments will be manually graded, but most of them will be autograded. You should have received an email invitation for Gradescope, but if not please let us know as soon as possible (preferably via Ed). | GitHub: Like in DSC 30, youâ€™ll access all course content (lecture slides and assignments) by pulling our course GitHub repository. The link to the repo is here. In most assignments, you wonâ€™t need to push anything to GitHub. However, you will need to push to GitHub as part of your Final Project, so youâ€™ll need to have an account by then. | . Note that we will not be using Canvas for anything this quarter. Development Environment . As soon as you are able to, go follow the steps in the Tech Support page of the course website to set up your development environment for the course. Forms . Please fill out the Welcome Survey to tell us a bit more about yourself and tell us if you need an alternate exam. ",
    "url": "/syllabus/#getting-started-",
    "relUrl": "/syllabus/#getting-started-"
  },"38": {
    "doc": "ğŸ“– Syllabus",
    "title": "Communication ğŸ’¬",
    "content": "This quarter, weâ€™ll be using Ed as our course message board. You will be added to Ed automatically; use the invite link in the section above if you werenâ€™t added. If you have a question about anything to do with the course â€” if youâ€™re stuck on a problem, didnâ€™t understand something from lecture, want clarification on course logistics, or just have a general question about data science â€” you can make a post on Ed. We only ask that if your question includes some or all of an answer (even if youâ€™re not sure itâ€™s right), please make your post private so that others cannot see it. You can also post anonymously to other students if you prefer. Course staff will regularly check Ed and try to answer any questions that you have. Youâ€™re also encouraged to answer questions asked by other students. Explaining something is a great way to solidify your understanding of it! . Please donâ€™t email individual staff members, just make a private or public Ed post instead. ",
    "url": "/syllabus/#communication-",
    "relUrl": "/syllabus/#communication-"
  },"39": {
    "doc": "ğŸ“– Syllabus",
    "title": "Course Components ğŸ",
    "content": "Lectures . Lectures will be held in-person on Tuesdays and Thursdays from 11:00-12:20PM in SOLIS 104. Lectures will be podcasted. New In the Welcome Survey at the start of the quarter, you will have the option of opting into lecture attendance or opting out. If you choose to opt in, lecture attendance will be worth 5% of your overall grade. If you choose to opt out, attendance will be worth 0%, and your midterm and final exam grade will each be worth 2.5% more. To receive lecture attendance for a given week, you must attend and participate in the in-lecture activities for both lectures that week. Each lecture will be weighted equally towards the lecture attendance portion of your grade, with 4 missed lectures allowed without penalty. Lecture notebooks will be your main resource in this class. You can access them, along with all course materials, by pulling from the course GitHub repository. We will also link HTML previews of each lecture notebook from the course homepage; you can use these to annotate the lecture notebooks with a tablet, if youâ€™d like. Supplementary readings (which are different from pre-lecture readings) will primarily come from Learning Data Science, a textbook written by Sam Lau. It can be found at learningds.org. Some readings will come from notes.dsc80.com, a set of notes that were originally written to supplement DSC 80. Supplementary readings are not required, in that you wonâ€™t be tested on anything that appears only in the readings but not in lectures or assignments, but you should still complete them to supplement your understanding! . Discussions . We will not be using the scheduled discussion times for this course. Instead, we will post Exam Prep worksheets with suggested questions from past exams. Labs . There will be 9 lab assignments due weekly throughout the quarter. Each lab assignment will consist of a mixture of coding and free response questions. Coding questions will ask you to fill in the body of a function. Public tests are usually provided so that you can make sure that you're on the right track (similar to DSC 20). However, your submissionâ€™s final score will use a private autograder with hidden tests. Each lab is worth the same amount, but the lowest lab will be dropped when calculating your final score. You will access labs (and projects) by pulling the course GitHub repository. Projects . There will be 4 projects due throughout the quarter. Like labs, projects consist of coding and free response questions. As their name implies, however, projects are more open-ended and allow you to simulate applying your data science skills in practical situations. You can think of the projects as being mini-take-home-exams that track your practical skills throughout the quarter (whereas the exams themselves test for conceptual understanding). Projects are due bi-weekly. However, the week before a project is due, there will often be a project checkpoint. This checkpoint will ensure that you're on-track to complete the project on time, and should (hopefully) be a source of easy points. The Final Project will be due during finals week and can be thought of as a practical component of the Final Exam. Note that, unlike labs, the lowest project score is not dropped. Working in Pairs . You may work together on projects (and projects only!) with a partner. If you work with a partner, you are both required to actively contribute to all parts of the project. You must both be working on the assignment at the same time together, either physically or virtually on a Zoom call. You are encouraged to follow the pair programming model, in which you work on just a single computer and alternate who writes the code and who thinks about the problems at a high level. In particular, you cannot split up the project and each work on separate parts independently. If you work with a partner: . | Only one partner needs to submit the project on Gradescope; this partner should add the other partner to their submission. | You must also submit the checkpoint together. | You and your partner will receive the same score on any submissions you make together. | . If you are unhappy with your partnership (e.g., if your partner does not keep in touch, does not come prepared to work on the assignment, or does not seem to be engaged in the process), please first address your concerns to your partner, and try to agree on what should be done to make the partnership work well for both of you. If that approach is not successful, explain the issues to the instructors, who will work with you and your partner to improve the situation. You may use different partners on different projects. Note that you may not work with partners on lab assignments, however youâ€™re encouraged to discuss all assignments with others at a conceptual level in office hours and study groups. Office Hours . To get help on assignments and concepts, course staff will be hosting several office hours per week. All office hours will be held in person. See the Calendar tab of the course website for the most up-to-date schedule and instructions. Weekly Schedule . To summarize all of the events and deadlines, refer to this general weekly schedule (which is subject to change in any given week): . | Monday | Tuesday | Wednesday | Thursday | Friday | . | Â  | Lecture | Â  | Lecture | Â  | . | Â  | Â  | Lab due | Â  | Project due | . ",
    "url": "/syllabus/#course-components-",
    "relUrl": "/syllabus/#course-components-"
  },"40": {
    "doc": "ğŸ“– Syllabus",
    "title": "Exams ğŸ“",
    "content": "This class has one Midterm Exam and one Final Exam. Exams are cumulative, though the Final Exam will emphasize material after the Midterm Exam. | Midterm Exam: Tuesday Feb 11, 11am-12:20pm in SOLIS 104 (during lecture) . | Final Exam: Thursday, March 20th, 11:30AM-2:30PM. Location is TBD . | . Both exams will be administered in-person. If you have conflicts with either of the exams, please let us know on the Exam Accommodations Form. Redemption Policy . The Final Exam will consist of two parts: a â€œMidtermâ€ section and a â€œpost-Midtermâ€ section. If you do better on the â€œMidtermâ€ section of the Final Exam than you did on the original Midterm Exam, your score on the â€œMidtermâ€ section will replace your original Midterm Exam score. This lowers the stakes of the Midterm Exam and gives you two opportunities to demonstrate your understanding of the content from the first half of the course. This also means that you can miss the Midterm Exam for any reason and have the score be replaced by your score on the â€œMidtermâ€ section of the Final Exam (though we do not recommend this). You must take the Final Exam to pass the course. ",
    "url": "/syllabus/#exams-",
    "relUrl": "/syllabus/#exams-"
  },"41": {
    "doc": "ğŸ“– Syllabus",
    "title": "Policies ğŸ’¯",
    "content": "Grading . Here is how weâ€™ll compute your grade: . | Component | Weight | Notes | . | Labs | 20% | 2.5% per lab, lowest dropped | . | Projects | 25% | 5% each for Projects 1-3, 10% for Project 4 | . | Project Checkpoints | 5% | 1% each | . | Midterm Exam | 20% | see the Redemption Policy above | . | Final Exam | 30% | Â  | . | Lecture Attendance | 0% | If opted in, counts as 5% and reduces Midterm and Final Exam by 2.5% each. | . Late Policy . All assignments must be submitted by 11:59PM San Diego time on the due date to be considered on time. You may turn them in as many times as you like before the deadline and only the most recent submission will count, so itâ€™s a good habit to submit early and often. New We recognize that life is unpredictable, and that there are circumstances and emergencies that cannot be resolved immediately. To account for this, you may request an extension for labs and projects by filling out the Extension Request Form. Filling out this form will grant you a 2-day deadline extension for an assignment with no grade penalty. (We would like this process to be automatic, but for now it requires a staff member to manually update your due date on Gradescope.) We are using this policy instead of slip days this quarter to help identify and support students who may be encountering difficult circumstances, so if we notice that you have used the form many times during the quarter, course staff will schedule a meeting with you to come up with a plan for completing the rest of the course before approving further extension requests. If you submit an assignment late and didnâ€™t fill out the Extension Request Form before the assignment deadline, the submission will not be accepted and you will receive a 0. Note that we will not grant deadline extensions for Project Checkpoint deadlines and the Final Project deadline. Redemption Policy for Labs and Projects . New Labs 1-9 and Projects 1-3 have a redemption policy to make up for lost points on your original submission. After the original deadline passes for an assignment, we will publish the autograder result for your latest submission before the assignment deadline. You may then submit the assignment within a week of the original assignment deadline to redeem up to 80% of the points you lost on the original submission. Example: Suppose that after grades were released for the Project 1 deadline, Sam receives a 90% since he lost points on a few hidden tests. He fixes the bugs, resubmits, and his new submission gets a 100%. His final grade for Project 1 would then be 98% (0.8 * (100% - 90%) + 90% = 98%). Note that this redemption policy does not apply to project checkpoints, or the Final Project. Regrade Requests . Most of the labs and projects are autograded, but some questions are manually graded. If you feel that there in an error in the autograder or that the manual grader has made a mistake, you may submit a regrade request within two days of the grades being released. If you do not submit a regrade request within two days, your original grade will be final. Regrade Requests for Manually Graded Problems . To submit a regrade request for a manually graded problem, make the regrade request directly on Gradescope. Note that part of your grade is clarity, so if your answer was mostly right but unclear you may still not be eligible for full credit. Regrade Requests for Autograded Problems . The autograder is very picky: it expects your assignments to have exactly the correct file names, all functions must be named correctly, etc. If these are wrong, your code may not run and the autograder may assign zero points. This is a grading catastrophe ğŸ˜§. Grading catastrophes are preventable! After submitting your assignment, always wait around to see the output of the Gradescope grader and ensure that it runs properly. Also, be sure to submit your assignment (or at least part of it) to Gradescope with enough time before the deadline to get help if there is a strange autograder problem. Incompletes . In the unfortunate circumstance that you become sick, suffer a loss, or otherwise experience a significant setback that is outside of your control, you may be eligible for an Incomplete grade, which allows you to complete the rest of the work at a later time. If you are experiencing challenges due to circumstances outside your control, please contact me ASAP and we can discuss the best course of action. Note that an Incomplete does not allow you to re-do work that has already been completed, only to do work that hasnâ€™t been completed, so itâ€™s best to reach out right away. A note on letter grades . The following is adapted from CSE 160 at the University of Washington. Grading for this class is not curved in the sense that the average is set at (say) a B+ and half of the class must receive a grade lower than that. If everyone does well and shows mastery of the material, everyone can receive an A (this would be awesome!). If no one does well (this is unlikely), then everyone can receive a C. Grading for this class is curved in the sense that we do not have a pre-defined mapping from project and exam scores to a final GPA. There is no pre-determined score (e.g., 90% of all possible points) that earns an A or a B or a C or any other grade. To determine the final grade, we will ask questions like â€œDid this student master the material?â€. With that said, grades will not be any stricter than the standard grading scale (where an A+ is a 97+, A is 93+, A- is 90+, etc). For instance, the threshold for an â€œAâ€ will never be higher than 93%. Try your best not to worry about grades, and weâ€™ll reciprocate by being fair. Weâ€™re in this together ğŸ˜. ",
    "url": "/syllabus/#policies-",
    "relUrl": "/syllabus/#policies-"
  },"42": {
    "doc": "ğŸ“– Syllabus",
    "title": "Collaboration Policy and Academic Integrity ğŸ¤",
    "content": "DSC 80 is known for being a rigorous but rewarding course. While you will be challenged this quarter, we will be offering you plenty of support through office hours and Ed. Make good use of these resources, and you will be able to succeed in this course. There is no excuse for cheating in this course. If you do cheat, we will enforce the UCSD Policy on Integrity of Scholarship. This means you will likely fail the course and the Dean of your college will put you on probation or suspend or dismiss you from UCSD. Students agree that by taking this course, their assignments may be submitted to third-party software to help detect plagiarism. Why is academic integrity important? . Academic integrity is an issue that is pertinent to all students on campus. When students act unethically by copying someoneâ€™s work, taking an exam for someone else, plagiarizing, etc., these students are misrepresenting their academic abilities. This makes it impossible for instructors to give grades (and for the University to give degrees) that reflect student knowledge. This devalues the worth of a UCSD degree for all students, making it imperative for the the campus as a whole to enforce that all members of this community are honest and ethical. We want your degree to be meaningful and we want you to be proud to call yourself a graduate of UCSD! . The UCSD Policy on Integrity of Scholarship and this syllabus list some of the standards by which you are expected to complete your academic work, but your good ethical judgment (or asking us for advice) is also expected as we cannot list every behavior that is unethical or not in the spirit of academic integrity. Ignorance of the rules will not excuse you from any violations. What counts as cheating? . In DSC 80, you can read books, surf the web, talk to your friends and the DSC 80 staff to get help understanding the concepts you need to know to complete your assignments. However, all code must be written by you (or, in the instance of projects, together with your partner). The following activities are considered cheating and are not allowed in DSC 80 (not an exhaustive list): . | Using or submitting code acquired from other students (except from your pair programming partner during projects), the web, or any other resource not officially sanctioned by this course | Posting your code online, including on Ed, unless privately to instructors only | Having any other person complete any part of your assignment on your behalf | Completing an assignment on behalf of someone else | Providing code, exam questions, or solutions to any other student in the course | Splitting up project questions with your pair programming partner and each working on different questions | Collaborating with others on exams | . The following activities are examples of appropriate collaboration and are allowed in DSC 80 (not an exhaustive list): . | Discussing the general approach to solving labs or projects | Talking about problem-solving strategies or issues you ran into and how you solved them | Discussing the answers to exams with other students who have already taken the exam after the exam is complete | Using code provided in class, by the textbook or any other assigned reading or video, with attribution | Google searching for documentation on Python or pandas | Working together with other students on assignments without copying or sharing answers | Posting a question about your approach to a problem on Ed, without sharing your code | . The best way to avoid problems is by using your best judgement and remembering to act with Honesty, Trust, Fairness, Respect, Responsibility, and Courage. Here are some suggestions for completing your work: . | Donâ€™t look at or discuss the details of another studentâ€™s code for an assignment you are working on, and donâ€™t let another student look at your code. | Donâ€™t start with someone elseâ€™s code and make changes to it, or in any way share code with other students. | If you are talking to another student about an assignment, donâ€™t take notes, and wait an hour afterward before you write any code. | . Use of Generative Artificial Intelligence . Generative Artificial Intelligence (GenAI) describes tools, such as ChatGPT and GitHub Copilot, that are trained to generate responses to user-defined prompts, or questions. The existence of such tools is a major milestone in machine learning, and an impressive application of data science in the real world. Our course policy on the use of GenAI tools for coursework is simple: you may use these tools to build an understanding of course material and to assist you on assignments, keeping in mind that no tool is a substitute for a strong understanding of course concepts. Be mindful of how you are using GenAI tools. These tools can be very useful to help you preview material before lecture, summarize material after lecture, explain concepts you didnâ€™t understand, and explore how different concepts are related. â€œExplain it like Iâ€™m fiveâ€ can be a helpful prompt to give you a basic understanding of new concepts before being exposed to them in lecture. Consolidating your knowledge after learning something new and relating it to other things you know is important for learning and retention. Unfortunately, GenAI tools are not a consistently reliable source of quality information. Because of how GenAI tools are trained, they often provide answers and write code that look correct, but arenâ€™t actually correct. A goal of your education is to develop an ability to identify and produce information that actually is correct and doesnâ€™t just sound correct. Human supervision of GenAI tools is always necessary. Proceed with caution when using tools to assist you with your assignments. DSC 80 is a foundational class for your study of data science; you need to master the skills and concepts of this course if you want to use data science effectively. Through exams, you will be tested on your independent ability to apply course material to novel problems. Labs and projects are meant to prepare you for these assessments, so overreliance on GenAI for assignments will rob you of opportunities to learn and make it hard for you to perform well on exams. If you do use GenAI to assist you on assignments, keep these guidelines in mind: . | Design your prompts carefully. Donâ€™t just ask one question; ask a follow-up question based on the output to the first. To use these tools effectively, you need to engineer your prompts carefully. | Test the outputs. GenAI tools can and do make mistakes, and being able to verify the correctness of a proposed answer is an important skill for you to develop. Validate the output against course-provided references, or follow up with a search on Google or Stack Overflow. Remember that GenAI tools provide crowdsourced likely answers, not necessarily correct answers. | Donâ€™t submit any code that you donâ€™t understand, or that uses content not taught in this class. In our experience last quarter, students who used ChatGPT to help with assignments ended up with code that was difficult for both them and the teaching staff to understand. If you answer questions with out-of-scope content, you are not practicing the foundational skills that the course is meant to teach you. Be careful! | . If your assignment submission includes any content generated by an AI tool, it should be cited to acknowledge the source of the material. In each assignment, you will be provided with a space to explain and reflect on your use of GenAI tool(s). ",
    "url": "/syllabus/#collaboration-policy-and-academic-integrity-",
    "relUrl": "/syllabus/#collaboration-policy-and-academic-integrity-"
  },"43": {
    "doc": "ğŸ“– Syllabus",
    "title": "Support ğŸ«‚",
    "content": "Accommodations . From the Office for Students with Disabilities (OSD): . OSD works with students with documented disabilities to review documentation and determine reasonable accommodations. Disabilities can occur in these areas: psychological, psychiatric, learning, attention, chronic health, physical, vision, hearing, and acquired brain injuries, and may occur at any time during a studentâ€™s college career. We encourage you to contact the OSD as soon as you become aware of a condition that is disabling so that we can work with you. If you already have accommodations via OSD, please make sure that we receive your Authorization for Accommodation (AFA) letter by the end of Week 1 so that we can make arrangements for accommodations. Share your AFA letter with the instructor and the Data Science OSD Liaison, who can be reached at dscstudent@ucsd.edu. Diversity and Inclusion . We are committed to an inclusive learning environment that respects our diversity of perspectives, experiences, and identities. Our goal is to create a diverse and inclusive learning environment where all students feel comfortable and can thrive. If you have any suggestions as to how we could create a more inclusive setting, please let us know. We also expect that you, as a student in this course, will honor and respect your classmates, abiding by the UCSD Principles of Community. Please understand that othersâ€™ backgrounds, perspectives and experiences may be different than your own, and help us to build an environment where everyone is respected and feels comfortable. Campus Resources . If there is an issue you feel uncomfortable speaking with us or are searching for help on a specific concern, there are several campus resources available to you, including: . | UCSD Counseling and Psychological Services (CAPS) | Office for Students with Disability (OSD) | Office for Prevention of Sexual Harrassment and Discrimination | . Food Support for Students: . If you are skipping and stretching meals, or having difficulties affording or accessing food, you may be eligible for CalFresh, Californiaâ€™s Supplemental Nutrition Assistance Program, that can provide up to $292 a month in free money on a debit card to buy food. Students can apply at: https://benefitscal.com/r/ucsandiegocalfresh. Also, The Hub Basic Needs Center at UCSD exists to empower all students by connecting them to resources for food, stable housing and financial literacy. Visit their site at https://basicneeds.ucsd.edu. ",
    "url": "/syllabus/#support-",
    "relUrl": "/syllabus/#support-"
  },"44": {
    "doc": "ğŸ“– Syllabus",
    "title": "Acknowledgements ğŸ™",
    "content": "This offering of DSC 80 builds off of prior offerings by Sam Lau, Tauhidur Rahman, Suraj Rampure, Justin Eldridge, Marina Langlois, and Aaron Fraenkel. Along with the help of their tutors and TAs, they developed much of the content that we will use in this course. ",
    "url": "/syllabus/#acknowledgements-",
    "relUrl": "/syllabus/#acknowledgements-"
  },"45": {
    "doc": "ğŸ“– Syllabus",
    "title": "ğŸ“– Syllabus",
    "content": " ",
    "url": "/syllabus/",
    "relUrl": "/syllabus/"
  },"46": {
    "doc": "ğŸ™‹â€â™‚ï¸ Tech Support",
    "title": "ğŸ™‹â€â™‚ï¸ Tech Support",
    "content": " ",
    "url": "/tech_support/",
    "relUrl": "/tech_support/"
  },"47": {
    "doc": "ğŸ™‹â€â™‚ï¸ Tech Support",
    "title": "Table of contents",
    "content": ". | Introduction | Environments and Package Managers | Replicating the Gradescope Environment . | Step 1: Install mamba | Step 2: Download environment.yml | Step 3: Create a new conda environment | Step 4: Activate the environment | . | Working on Assignments . | Activating the conda environment | Using Git | Choosing a Text Editor or IDE | Using VSCode to Run Jupyter Notebooks | . | . ",
    "url": "/tech_support/#table-of-contents",
    "relUrl": "/tech_support/#table-of-contents"
  },"48": {
    "doc": "ğŸ™‹â€â™‚ï¸ Tech Support",
    "title": "Introduction",
    "content": "In DSC 10, you worked on assignments on DataHub, a computing platform that already had all of the Python packages you needed installed. But in the real world, youâ€™ll be expected to set up and maintain a Python environment locally â€“ that is, on your own computer â€“ and so thatâ€™s what weâ€™ll have you do here. Thatâ€™s right â€“ no DataHub! You already have experience writing and running code locally from DSC 20 and DSC 30; setting up your environment for DSC 80 will be slightly more involved than it was there, but most of these steps only need to be done once. There has been a lot written about how to set up a Python environment, so we wonâ€™t reinvent the wheel. This page will only be a summary; Google will be your main resource. But always feel free to come to a staff memberâ€™s office hours if you have a question about setting up your environment, using Git, or similar â€” weâ€™re here to help. This video walks through most of the steps here, but itâ€™s not a substitute for reading this page carefully. ",
    "url": "/tech_support/#introduction",
    "relUrl": "/tech_support/#introduction"
  },"49": {
    "doc": "ğŸ™‹â€â™‚ï¸ Tech Support",
    "title": "Environments and Package Managers",
    "content": "For this class, the software youâ€™ll need includes Python 3.12, a few specific Python packages, Git, and a text editor. Gradescope has an environment which it uses to autograde your work. You can think of an environment as a combination of a Python version and specific versions of Python packages that is isolated from the rest of your computer. In practice, developers create different environments for different projects, so that they can use different versions of packages in different projects. Weâ€™re going to have you replicate the environment Gradescope has on your computer. The reason for this is so that your code behaves the same when you submit it to Gradescope as it does when you work on it on your computer. For example, our Gradescope environment uses numpy version 1.21.2; if you install a different version of numpy on your computer, for example, you might see different results than Gradescope sees. How do you install packages, then? pip is a common choice, but even though itâ€™s widely used, it lacks built-in support for creating isolated environments. This limitation makes it challenging to maintain version consistency and avoid conflicts between packages. Consequently, we do not recommend relying solely on pip install for environment management, as it may inadvertently introduce incompatible package versions. conda, on the other hand, is a powerful tool that not only installs packages but also manages environments effortlessly. It allows you to create isolated environments and ensures compatibility among the packages within those environments. The tool weâ€™re going to use, though, is mamba, which is a wrapper around conda that is designed to be much faster. If you should need to install a new Python package, you can use the mamba command (once you have mamba installed). Inside the Terminal, type mamba install &lt;package_name&gt;, where &lt;package_name&gt; is replaced by the name of the package you want to install, and hit enter. However, you should only run mamba install once youâ€™ve entered your dsc80 environment â€“ more on this below. ",
    "url": "/tech_support/#environments-and-package-managers",
    "relUrl": "/tech_support/#environments-and-package-managers"
  },"50": {
    "doc": "ğŸ™‹â€â™‚ï¸ Tech Support",
    "title": "Replicating the Gradescope Environment",
    "content": "Below, weâ€™re going to walk you through how to create the same environment that Gradescope uses. Step 1: Install mamba . The way to do this depends on whether youâ€™re on a Unix-like platform (macOS or Linux) or on Windows. Unix-like platforms (macOS or Linux): . | Download the mamba installer. To do this, open your Terminal and run: . curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\" . This will place a file named something like Miniforge3-Darwin-arm64.sh wherever you ran the command. If you get an error saying command not found: curl, replace curl -L -O with wget and re-run the same command. | Run the installer. To do this, immediately after the last command, run: . bash Miniforge3-$(uname)-$(uname -m).sh . | . Windows: . | Download the Windows install script from here under â€œMiniforge3.â€ The file should be named Miniforge3-Windows-x86_64.exe or similar. | Run the downloaded .exe file. Follow the prompts, taking note of the options to â€œCreate start menu shortcutsâ€ and â€œAdd Miniforge3 to my PATH environment variableâ€. The latter is not selected by default due to potential conflicts with other software. Without Miniforge3 on the path, the most convenient way to use the installed software (such as commands mamba) will be via the â€œMiniforge Promptâ€ installed to the start menu. | Run the following command in your Anaconda Prompt: start /wait \"\" Miniforge3-Windows-x86_64.exe /InstallationType=JustMe /RegisterPython=0 /S /D=%UserProfile%\\Miniforge3 . | . Make sure to run this command in the same folder that Miniforge3-Windows-x64_64.exe is! If thatâ€™s not the folder that your command line interface is looking in, youâ€™ll need to cd there first, e.g. cd C:\\Users\\surajrampure\\Desktop if that file is on your Desktop. Step 2: Download environment.yml . This file contains the necessary details to configure your environment. If you take a look at it, youâ€™ll see that it contains a specific Python version (python=3.12) along with specific package versions (like pandas==2.2.3 and requests==2.32.3, for example). Step 3: Create a new conda environment . Yes, we said conda environment, even though weâ€™re using mamba to create it. To create the environment, in your Terminal or Anaconda Prompt, run: . mamba env create -f environment.yml . Note that if you put environment.yml in your Downloads or Desktop folder, you should replace environment.yml with the path to the file, for example: mamba env create -f /Users/yourusername/Desktop/environment.yml. Otherwise, you might get an error saying environment.yml does not exist. Step 4: Activate the environment . To do so, run: . mamba activate dsc80 . Where did the name dsc80 come from, you might ask? We defined it for you at the top of environment.yml with name: dsc80. If you get an error saying mamba isnâ€™t defined, try closing and reopening your Terminal first and then rerunning the command. ",
    "url": "/tech_support/#replicating-the-gradescope-environment",
    "relUrl": "/tech_support/#replicating-the-gradescope-environment"
  },"51": {
    "doc": "ğŸ™‹â€â™‚ï¸ Tech Support",
    "title": "Working on Assignments",
    "content": "Activating the conda environment . The setup instructions above only need to be run once. Now, every time you work on DSC 80 assignments, all you need to do is run . mamba activate dsc80 . in your Terminal or Anaconda Prompt. If you need to install any packages into your dsc80 environment using mamba install, make sure to activate the environment first. If youâ€™re using VSCode, you should select the Python kernel corresponding to the dsc80 environment to use it. To open a Jupyter Notebook, use the jupyter notebook command in your Terminal or Anaconda Prompt. Using Git . All of our course materials, including your assignments, are hosted on GitHub in this Git repository. This means that youâ€™ll need to download and use Git in order to work with the course materials. Git is a version control system. In short, it is used to keep track of the history of a project. With Git, you can go back in time to any previous version of your project, or even work on two different versions (or \"branches\") in parallel and \"merge\" them together at some point in the future. We'll stick to using the basic features of Git in DSC 80. There are Git GUIs, and you can use them for this class. You can also use the command-line version of Git. To get started, you'll need to \"clone\" the course repository. The command to do this is: . git clone https://github.com/dsc-courses/dsc80-2025-wi . This will copy the repository to a directory on your computer. You should only need to do this once. Moving forward, to bring in the latest version of the repository, in your local repository, run: . git pull . This will not overwrite your work. In fact, Git is designed to make it very difficult to lose work (although it's still possible!). Merge Conflicts . You might face issues when using git pull regarding merge issues and branches. This is caused by files being updated on your side while we are also changing the Git repository by pushing new assignments on our side. Here are some steps you can follow to resolve them: . NOTE: Whenever working with GitHub pulls, merges, etc., itâ€™s a good idea to save your important work locally so that if you accidentally overwrite your files you still have the work saved. Save your work locally before following the steps below. | git status shows the current state of your Git working directory and staging area. Itâ€™s a good sanity check to start with. You will probably see your project and lab files that you have worked on. | git add . will add all your files to be ready to commit. | git commit -m \"some message of your choice\" will commit the files, with some description in the quotations. This can be whatever you want, it wonâ€™t matter. | . At this stage, if you git pull, it should work. You should double-check that you have new files, as well as that your old files are unchanged. If they are changed then you should be able to just copy-paste from your local backup. If this does not work then you may have merge conflicts, follow the next steps: . | git checkout --theirs [FILENAME] will tell git that whenever a conflict occurs in [FILENAME] to keep your version. Run this for each file with a conflict. | git add [FILENAME] to mark each file with a conflict as resolved. | git rebase --continue or git merge, depending on the setup. | . Choosing a Text Editor or IDE . In this class, you will need to use a combination of editors for doing your assignments: The Python files should be developed with a text editor (for syntax highlighting and running doctests) and the data/results should be analyzed/presented in Jupyter Notebooks. Below is an incomplete list of IDEs you might want to try. For more information about them, feel free to ask the course staff. If youâ€™re curious, Suraj uses VSCode to edit .py files and the vanilla Jupyter environment to edit notebooks. | The JupyterLab text editor: see below. Can be used to edit both notebooks and .py files. | VSCode: Microsoft Visual Studio Code. Currently very popular, and can also be used to edit both notebooks and .py files. | sublime: A favorite text editor of hackers, famous for its multiple cursors. A good, general-purpose choice. | atom: GitHubâ€™s editor. Pretty nice fully featured IDE. Can only work locally. | PyCharm (IntelliJ): Those who feel at home coding Java. Can only work locally. | nano: available on most unix commandlines (e.g. DataHub Terminal). If you use this for more than changing a word or two, you'll hate your life. | (neo)vim: lightweight, productive text-editor that might be the most efficient way to edit text, if you can ever learn how to use it. Justin Eldridgeâ€™s text editor of choice. | emacs: A text editor for those who prefer a life of endless toil. Endlessly customizable, it promises everything, but youâ€™re never good enough to deliver. | . Using VSCode to Run Jupyter Notebooks . Many students like to use VSCode to edit Jupyter Notebooks. If thatâ€™s you, then youâ€™ll need to make sure to activate your dsc80 conda environment within your notebook in VSCode. Hereâ€™s how to do that. | Open a Juypter Notebook in VSCode. | Click â€œSelect Kernelâ€ in the top right corner of the window. | Click â€œPython Environmentsâ€ in the toolbar that appears in the middle. | Finally, click â€œdsc80 (Python 3.12.6)â€. | . ",
    "url": "/tech_support/#working-on-assignments",
    "relUrl": "/tech_support/#working-on-assignments"
  }
}
